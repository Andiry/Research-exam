\section{File system} 
\label{sec:fs}

To efficiently make use of the NVMM and expose a user-friendly
interface to applications, system software developers need to consider
how to manage NVMM and work with existing space applications smoothly.
As NVMM can be treated as storage device, file system is a straight forward
way to manage the NVMM.

DRAM file systems such as ramfs and tmpfs can also run on NVMM, but they
do not support recovery capability, and file system consistency may be 
neutralized by
CPU caching and write reordering, thus they are not eligible NVMM file systems.
Most researches design NVMM file systems from scratch. 

\textbf{eNVy} eNVy~\cite{eNVy} is a early exploration of NVMM storage system
design. It proposes a system that attaches flash to the memory
bus, and using SRAM to achieve high bandwidth and low latency. At that time
there is no real byte-addressable NVMM technology, so eNVy uses flash memory
as main storage device. To expose the byte-addressable capability to 
applications, eNVy uses battery backed SRAM as a write buffer and performs
copy-on-write on flash memory pages. eNVy also stores the page table inside
SRAM, and perform cleaning in the background to guarantee write latency and
bandwidth. To do the cleaning efficiently, eNVy divide flash pages into
segments and lower the utilizations of hot segments to reduce the cleaning
time.

\ignore{
eNVy is not using real byte-addressable NVMM devices, but its implementation
shows insight for SSD designs. Current commercial SSD works mainly like
eNVy: SSD uses DRAM as write buffers, deploy FTL layer to do the page remapping
, and perform garbage collection in the background to provide available flash
pages and wear leveling.
}

\textbf{Conquest}~\cite{conquest} is a NVMM/Disk hybrid file system.
With the observation that most acesses are to small files and most storage is
comsumed by large files, Conquest puts all small files and metadata in NVMM,
while disks only hold the data content of large files. With Conquest, the most
frequest accesses to files are satisfied in NVMM and the overall performance is
improved.

\textbf{SCMFS}~\cite{scmfs} is a file system designed for NVMM that
reuses the memory
management module in the operating system to reduce the implementation
complexity. SCMFS allocates 
each file to a contiguous virtual address space
to simplify the read/write file operations, and combines the Linux kernel
\texttt{mfence} and \texttt{clflush} operations to flush the CPU cache in
range and sequence the write operations.

\ignore{
SCMFS adds a new address range to the operating system for the files resided
on the NVMM, and 
adds a new memory zone \emph{ZONE\_STORAGE} for NVMM space allocation. The
NVMM allocation function \texttt{nvmalloc()} comes from \texttt{vmalloc()}
so that the allocated pages have contiguous virtual address but not necessary
to be contiguous in physical address.

The author claims that running in virtual address helps to make the SCMFS
implementation simple and improve the performance, but it also increases
TLB misses. To address the issue of write reordering, SCMFS combines the
 Linux kernel
\texttt{mfence} and \texttt{clflush} operations to flush the CPU cache in
range and sequence the write operations. It may not be as fast as hardware
primitives, but the performance is acceptable for common workloads.

SCMFS stores all the virtual-physical page mapping information in the NVMM
to restore the mappings when system fails and reboots. However this is not
a scalable solution: the mapping table size increases with the file system
size, and the time to mount the entire file system also increases. SCMFS
resolves the issue with a lazy mapping, which maps only the metadata and inode
table during the mount, and file data is mapped in background.
Another scalability issue of SCMFS is that it has a fixed space for inode table,
which means the number of files it support is fixed, and it does not scale
well with NVMM size.
}

An evolution of SCMFS, \textbf{Superpage}~\cite{superpage} further improves
SCMFS performance by allocating larger pages and reducing TLB misses. Superpage
allocates normal 4KB pages for small files and file 
metadata while allocating 2MB superpages for larger files, and improve overall
performance with less TLB misses.

\textbf{BPFS}~\cite{BPFS} is a good example to show how to design a NVMM
file system, and demonstrates the challenges in the implementation.
BPFS guarantees that
each file system operation is performed atomically and in order.

BPFS provides these guarantees with software implementation and hardware
primitives. In software part, it uses \emph{short-circuit shadow paging}
to provide fine-grained copy-on-write implementations. It also proposes
two hardware primitives, \emph{atomic 8-byte writes} and \emph{epoch barriers}
to support the short-circuit shadow paging and improve the file system
performance.

Shadow paging is a common file system design to keep file system in consistent
state during system failure. To update data with shadow paging, file system
does not write new data in-place; instead, it makes a copy of the old data,
update the copy with new data, and then update the pointer in the upper layer
atomically to point to the new data. Shadow paging guarantees the update is done
atomically and file system is always in a consstent state.
 ZFS~\cite{zfs} and WAFL~\cite{wafl} are two file
systems that use shadow paging. However, traditional shadow paging cascades
the copy-on-write operation from the modification place up to the root of 
the file system tree, and hurts performance. With short-circuit shadow paging,
BPFS does not always need to propagate the change up to the root, and performs
shadow paging at fine granularity to improve the performance.

To short-circuit the shadow paging, BPFS uses the atomic 8-bytes write primitive
to update the pointer, so that the pointer update in the shadow paging is done
atomically, eliminates the need to cascade the copy-on-write operations.
8-byte is the size of pointer in 64bit systems, so if file system only needs to
update a pointer in a data block, then the update is atomic. To support
atomic 8-byte writes, BPFS adds a capacitor to each DIMM, so that it has enough
power to finish all the outstanding writes in the row buffers when the system
power fails.

\ignore{
To provide safety and consistency, file system also needs to ensure that write
is committed to persistent storage in program order. However, existing cache
architecture and memory controller may reorder the writes to DRAM to improve
performance. Operating systems provide \texttt{mfence} primitive to guarantee
that all CPUs have a consistent view of the memory, but it does not impose
any constraints on the write order to main memory. A NVMM file system must
provide write ordering to file system updates.

A simple way to provide write ordering is to flush the CPU cache line after
write operation, or uses write-through to bypass CPU cache directly. However,
these operations are expensive and undermines performance. Instead,}
 BPFS uses
epoch barrier to resolve the write reordering issue. 
An epoch is a batch of write operations,
and hardware guarantees that epochs are committed to NVMM in program order.
With the epoch barrier all the current writes cannot proceed until all the
writes in the previous epochs are committed to NVMM and file system consistency
is maintained. BPFS implements the epoch barrier primitive by adding counters,
bits and capacitors to CPUs.

With hardware support, BPFS implements a transactional file system for NVMM
and gains better performance than traditional file systems.


\textbf{PMFS}~\cite{PMFS} is another NVMM file system. It is aiming to
provide a
light-weight POSIX file system that exploits NVMM's byte-addressability to
avoid overheads of block layer and enable direct NVMM access by application.
PMFS only requires a simple hardware primitive to provide software enforceable
guarantees of durability and ordering of stores to NVMM and exposes the 
performance potential of NVMM to user applications.

The author analyzes several approaches to enforce consistency, and finds out
using NVMM as a write-back cacheable memory and explicitly flushing modified
data from CPU cache (\emph{clflush}) works well. However, current \emph{clflush}
implementation is strongly ordered and incurs performance penalty. Instead,
PMFS proposes a optimized \emph{clflush} implementation which has  weaker
 ordering
and hence improve performance. To provide the write ordering PMFS proposes
a hardware primitive, \emph{pm\_wbarrier} to guarantee durability of NVMM stores
already flushed from CPU caches.

As a file system, PMFS also provides consistency. BPFS uses a file-grained
shadow paging mechanism, while PMFS uses logging. There are two kinds of logging
in file system implementation, redo and undo logging. Undo logging in PMFS
requires a \emph{pm\_wbarrier} for each log entry while redo logging requires
only two \emph{pm\_wbarrier} in a single transaction. However, undo logging
is easy to implement and PMFS uses undo logging. Whenever there is an update
to the PMFS metadata, PMFS copies the original value to the log entry and
make it persistent before write the new value in-place. PMFS uses the
8-byte and 16-byte atomic updates of CPU to perform atomic in-place updates.
For file data updates, PMFS uses copy-on-write mechanism.

PMFS also supports write protection. PMFS leverages
the CPU's write protect control mechanism (CR0.WP) to open small short write
windows without changing the page tables and hence avoid the expensive
TLB flushes.

To expose the high bandwidth and low latency of NVMM to applications,
PMFS uses the Direct Access (DAX)~\cite{ext4dax} interface to support
memory-mapped
IO operations. With DAX support the NVMM pages are directly mapped into
application's address space and bypass the page cache and block layer.
In contrast, BPFS does not support \texttt{mmap}.
PMFS also supports large page size to decrease the page table size and
TLB entries. ~\cite{mmfs} is another implementation of NVMM file system
which supports DAX interface. It integrates memory management with file system
to manage NVMM, allocate NVMM in pages and directly map to application
address space to gain better performance.

\ignore{
\subsection{Conclusion} 
\label{sec:fs-conclude}
TBD
}

\textbf{Aerie}~\cite{Aerie} tries to reduce the software overhead to access NVMM
by redesigning the file system interfaces. Aerie exposes the file system
data stored in NVMM directly to user-mode programs. Applications link to
a user space library that provides local access to data and a OS kernel
module provides coarse-grained allocation and protection. Applications can
modify file data directly while contact the file system service to update
metadata.

Aerie consists of three components: an untrusted user mode library (libFS),
a trusted file system service (TFS), and a kernel module NVMM manager. libFS
handles data access, TFS provides service for metadata updates and concurrency
control among processes, and NVMM manager provides storage allocation mechanism.
libFS uses RPC to communicate with TFS, and TFS works as lock manager to 
assign locks to applications using libFS, and it can revoke locks if needed.
As TFS resides in the user space, most metadata updates do not go to the kernel
and eliminate most mode switch overheads.

The protection is provided by both NVMM manager and TFS, which divide the NVMM
into extents, embedding them with permission information, and grant applications
to the extents accordingly. To reduce RPC communication overhead, Aerie
uses a batched and delayed metadata update mechanism, only when updates need to
be visible to other clients. Another optimization Aerie does is to using
a in-memory cache of path names to speed up name resolution.

Aerie designs a file system, PXFS wich complies with POSIX semantics.
To further show the impact of POSIX interfaces on performance, they also design a flat
key-value store storage system (FlatFS), which applications access file
through a simple put/get/erase interface. FlatFS is simpler, avoiding the
overhead of name resolution and accelerates file sharing. The evalution shows FlatFS
outperforms PXFS by up to 109\%, leading us to rethink the file system interface design.

