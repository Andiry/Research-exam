\section{DRAM extensions} 
\label{sec:migration}

Although NVMM has low power consumption and high density comparing to DRAM,
it is still not good enough to replace DRAM entirely due to the slow write
and low endurance. We expect in the future DRAM and NVMM
will reside on the system memory bus together. Thus, how to design a
NVMM/DRAM hybrid system becomes an interesting
topic and several researches contributes to this area.

The most common idea is to direct write-intensive pages to DRAM and read-only
or read-intensive pages to NVMM.
Mogul et al.~\cite{Mogul} invesitgates the necessary operating system support
for NVM+DRAM hybrid main memory, concludes that OS should decide page
migration issue to avoid endurance problems and high write latency of NVMM
devices.
Park et al.~\cite{pcmalloc} allocate heap and stack segments with DRAM pages
which are
updated frequently, and allocate NVMM pages to text/bss/data segments as
they are updated infrequently.
\textbf{PCM3D}~\cite{pcm3d} combines the DRAM
and NVMM in a large flat memory region and migrate write-intensive pages
to DRAM. The migration is performed in operating system and target the
frequently written pages, while leave the read-intensive pages in NVMM.
\textbf{PDRAM}~\cite{pdram} introduces a hybrid PRAM and DRAM main memory
system, 
relies on a hardware memory controller to log the write counts to each page,
while a software module swap pages between DRAM and NVMM and does wear-leveling.
\textbf{RaPP}~\cite{RaPP} implements the page migration inside the memory
controller to monitor access patterns and migrate pages between NVMM and DRAM. 

\ignore{
Mogul. et al~\cite{Mogul} describes the necessary operating system support for
NVM+DRAM hybrid main memory. The paper explores two possible NVM devices:
flash memory and PCM device to reside on the memory bus. As flash memory
is block-addressable with high write latency and low endurance, the paper
proposed that only read-only and read-heavy pages should be stored in flash
memory, and page is migrated between DRAM and flash memory, also garbage
collection and wear leveling is needed. For PCM devices, the system design
is simpified as PCM is byte-addressable, garbage collection is not necessary
and wear-leveling is simpler. The paper concludes that for a hybrid main
memory system, OS should decide page migration issue to avoid endurance
problems and high write latency of NVMM devices.

\textbf{RaPP}~\cite{RaPP} implements the page migration inside the memory
controller
to monitor access patterns and migrate pages between NVMM and DRAM. Rapp
proposes a complicate memory controller design with a rank-based page
placement policy. Depending on the access patterns to pages, RaPP create
multiple page queues to store different rank page frames, place
performance-critical pages and frequently written pages in DRAM, place
non-critical pages and rarely written pages in PCM and spread writes
to PCM across many physical frames. By putting the page placement policy
inside memory controller, RaPP achieves better performance than software
solution, but with more sophisticated hardware design.
}

Qureshi et al.~\cite{PCMHierarchy} proposes a PCM-based hybrid main memory
system. To get the best capacity and latency trade-off, the system applies
large capacity of PCM to hold most of the pages during program execution
and uses a DRAM buffer to absorb the writes. 

The system tries to minimize writes to the NVMM to improve both performance
and endurence. Cold data is fetched from disks directly to the DRAM buffer
without going to the NVMM; only when the DRAM buffer is full, dirty page is 
evicted to the NVMM. This is called \emph{lazy-write organization}. Another
optimization is \emph{line-level writeback}. Instead of writing the whole dirty
page to the NVMM, the system records dirty data by 256 bytes linesize
granularity, with a dirty bit per line in the tag store of DRAM buffer.
When evict a dirty page to NVMM, only the dirty lines are actually written to
the NVMM. Both lazy-write and line-level writeback try to minimize the writes
to the NVMM to extend the endurance.

The system also supports fine-grained wear-leveling for NVMM. A random 4-bit 
number is generated when a page is fetched from disk, and all reads and writes
to the NVMM is shifted by the rotate number to make
the writes uniform across all 16 lines in the page.
This helps to boost the system endurance by 3\x{}.

Some applications only aim to use NVMM as DRAM and do not require consistency
support. To benefit from this usage model, systems are proposed that
manage NVMM both as volatile and non-volatile memory.
\textbf{Memorage}~\cite{memorage} argues that NVMM-based systems will have large
capacity of NVMM that need to be managed as both memory and storage in
an integrated manner to make the best performance and trade-offs. For exmaple,
fast, high-endurance SLC PCM can be used as main memory
while slow and low-endurance MLC PCM works as storage device.

Memorage manages both NVMM memory and storage resources, develops a dynamic
strategy to expand and shrink memory capacity according to system status. 
With large capacity NVMM sitting on memory bus, Memorage has two benefits:
1. When system is in low memory state, Memorage does not swap unused pages
to disk, instead it borrows pages from NVMM storage device to cope with
memory shortage.
2. As long as NVMM storage device has extra capacity, Memorage can use it
to relax the write endurance of NVMM main memory and facilitate wear-leveling.

\textbf{NVM Duet}~\cite{duet} provides flexible solution to the applications
which
have different expectations of NVMM: some are seeking the persistency, while
others just use NVMM as normal DRAM. As consistency is achieved via serializing
write operations and CPU cache flushing, differentiating between the two usages
makes sense to improve overall system performance.

NVM Duet provides a hardware/software interface that enables memory controller
to differentiate between the two types of NVMM usage as working memory and
persistent store. When NVMM is working as memory, CPU cache flushing is
eliminated as applications do not require persistency. Also memory controller
can fully exploit the bank-level parallelism while respecting the write order 
specified by programmers to guarantee the consistency of persistence store.
NVM Duet also proposes a new PCM architecture that provides dual retention
guarantees to meet the durability requirement of persistence store while relax
the retention requirement of working memory to reduce write latency. It also
applies a smart refresh mechanism to eliminate unnecessary refresh operations
by checking the resistance level. NVM Duet is another exmaple of exploiting
optimizations from the different NVMM usages.

\textbf{NVMCap}~\cite{NVMCap} resembles NVM Duet that it also divides
the applications into two categories: those use NVMM for capacity without
persistence (NVMCap), and those use the persistence (NVMPersist). Cache flushing
are eliminated for NVMCap applications.
 
NVMCap proposes several optimizations to current NVMM systems.
Current systems keep metadata in NVMM to recover from system failure.
These metadata are updated frequently during system running and make
them persistent adds allocator overhead. To overcome the issue, NVMCap
keep metadata in DRAM buffer, and log the updates to NVMM to reduce NVMM
writes. To further improve the allocator performance, the system allocates
physically contiguous pages in batch and pre-allocate them to applications 
to reduce cache sharing impact. While still using logging for NVMPersist
applications, the logging is also divided into two categories: fine-grained
word log for metadata updates, and coarse-grained object log for object
updates. word log contains the full metadata, while object log indicates
the address and size of the object. This reduces log size and NVMM writes for
object as eliminates the need to log the entire object.

\textbf{Summary} As NVMM has higher density and lower power consumption,
applications may use it as DRAM extension.
To hide the high write latency of NVMM and
improve endurance, OS MMU should put write-intensive pages in DRAM, and migrate
pages between DRAM and NVMM dynamically. This requires write count logging and
page migration mechanism, which can be implemented in both hardware and
software. As NVMM pages can be accessed from user space directly, we claim 
that at least write count logging should be implemented in hardware.

When applications access NVMM as DRAM, the persistency guarantee can be relaxed.
NVMM systems can benefit from this to improve performance. However, this
requires applications to specify their usage pattern of NVMM. 
