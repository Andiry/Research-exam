\section{Access model} 
\label{sec:access}

With the emergence of non-volatile memory, the accessing model needs
to be re-considered. Accessing non-volatile memory addresses several
challenges: how to create and manage non-volatile memory, how to ensure
consistency during system failures, and how to avoid common programming
bugs, such as dangling pointers, multiple free()s, and locking errors.

\nvh{}~\cite{nvheaps} and \mnem{}~\cite{mnemosyne} try to address these issues
and propose solutions to the non-volatile memory accessing models.

\nvh{} creates a lightweight system that exposes the performance of the
non-volatile memories and also ensures safety in the presence of
application and system failures by avoiding common programming errors.
It provides safe access to persistent objects with pointer safety and
flexible ACID transactions, and makes persistent objects easy to program.

\nvh{} is implemented as a C++ library under Linux. It manages
the NVMM with a memory allocator, and uses a gabbage collector to recycle
persistent objects. It also provides transaction support and atomic sections 
to provide concurrency and consistency against system failure.

\nvh{} addresses the volatile pointer and non-volatile pointer issue by
only allowing pointers within a single NV-Heap (intra-heap NV-to-NV pointers)
and pointers from volatile memory to a NV-Heap (V-to-NV pointers). This is
because pointers from an NV-Heap to volatile memory (NV-to-V pointers) do
not make sense when the system reboots, and inter-heap NV-to-NV pointers become
unsafe if the NV-Heap that contains the object is not available. By enforcing
this pointer restriction and checking pointers dynamically \nvh{} provides
pointer safety guarantee.

\nvh{} uses reference count to track dead objects and recycle them. When 
the reference count to an object reduces to zero, the deallocation routine 
atomically calls the destructor and deallocates the object. As the deallocation
is atomic, the deallocator can resume work after system failure without error.

\nvh{} uses undo log to provide ACID transaction support. Whenever application
wants to modify an object, \nvh{} makes a copy of the original object to
the write log and then updates the object. If system fails during
the transaction, \nvh{} rolls back the object to its original copy by using
the copy in the log.

\nvh{} exists as normal files in a file system, and it uses DAX
\texttt{mmap()} to map the NVMM pages directly into the application's
address space. Application must explicitly declare persistent objects in
\nvh{} and access the objects with caution. This adds overhead to the
programming but \nvh{} exerts to export simple interface for programmer
and hide the details.

\mnem{} is also addressing the management and persistency issue
of system failure for NVMM systems. \mnem{} provides three key services
that simplify the usage of NVMM. First, it provides \emph{persistent
memory regions}, which can be created automatically to store persistent 
variables or allocated dynamically. Second, \mnem{} provides persistence
primitives, low-level operations that support atomic data updates. Finally,
\mnem{} provides transaction support that enables consistent in-place
updates of persistent objects. \mnem{} does not provide type-safe pointers
and gabbage collection that \nvh{} provides, but it does not require
modification to the processor.

To bypass the CPU cache and makes writes to NVMM consistent, \mnem{} uses
\emph{write-through stores} that writes data to NVMM directly. It also
uses \emph{fense} to provide write ordering and \emph{flush} to flush
cacheline to the NVMM. \mnem{} uses redo logging to provide transaction support
as it does not require write ordering to log before every memory update.

\mnem{} consists of three parts: a kernel module to expose and virtualize NVMM, 
libraries to implement persistent regions and transaction system, and
a compiler to support persistent variables and transactions. To use \mnem{},
programmer uses \texttt{pstatic} to declare a persistent variable in the NVMM,
and calls \texttt{pmap} to tell \mnem{} to allocate persistent regions in NVMM
and map them to application's address space. From the view of programmer,
\mnem{} works similar to \nvh{}: they both require programmer to explicitly
declare persistent objects and modify their softwares.

\mnem{} has some limitations. It does not support persistent region sharing
between processes. Also it does not address the wearout issue of NVMM.

CDSMM~\cite{CDSMM} also presents the memory management module design for NVMMs.
Rather than performance, it focuses on security and protection: it introduces
techniques for 1) robust wear-aware memory allocation, 2) preventing for 
erroneous writes, and 3) consistency-perserving updates that are cache-efficient.

As hardware wear-leveling has performance issues, CDSMM presents a
software memory allocator that always allocate new blocks rather than reusing
old freed blocks to average the writes to each block. To reduce metadata
writes, CDSMM puts important information in DRAM and sync to NVMM only required.It also adds checksum to prevent stray writes from modifying metadata.

To protect NVMM pages CDSMM uses system call \texttt{mprotect}, but performs
it in a batch way so that to reduce mode switch overhead, kernel structure
modifying overhead and TLB flushes. CDSMM also proposes adding counter to CPU
cache line so that application can use command to track data is either in 
CPU cache or persistent on NVMM, and resolves the persistency and write ordering
issue. 

The disadvantage of CDSMM is that it requires both software and hardware
modifications, adds significant overhead to software programming. The paper
shows a B+ tree example and in order to apply CDSMM design, it does not only 
adds much more software efforts, but also change the definition of B+ tree:
the values in a node is not in sorted order and it takes linear time to search
value in a node. CDSMM has several interesting ideas about NVMM protection, but
to integrate the design in applications is still difficult.

CDDS~\cite{cdds} proposes a consistent and durable data structure for NVMM.
Instead of logging, CDDS works like snapshot file systems such as
WAFL~\cite{wafl} and ZFS~\cite{zfs}, uses version control to organize data,
makes it possible to rollback to earlier persistent versions during crash
recovery.

CDDS uses a combination of \texttt{mfence} and \texttt{clflush} to make sure
data in CPU cache is written to NVMM persistently, just like other systems.
However, it does not use logs to provide transaction guarantees. In the B-tree
created by CDDS, each node has a start version and end version, to show the
life period of the node. When a node is created, its start version is set
to the current B-tree version. When an update to the B-tree makes the node
not alive, the end version is set to the B-tree version accordingly.
Every update to the B-tree results in a creation of
a new version, and read only returns the newest version. A garbage collector
is running in background, eliminates those dead data and clear space for
new data. During crash recovery, nodes newer than the most recent consistent
version are removed and the older nodes are recursively for partial updates,
make sure the tree is indentical to the most consistent version.

CDDS shows the version-based design of the data structure on persistent
memory. However, the implementation is complicated and specific. We still
need to find out a more generic way to access the NVMM.

Aerie~\cite{Aerie} tries to reduce the software overhead to access NVMM
by redesigning the file system interfaces. Aerie exposes the file system
data stored in NVMM directly to user-mode programs. Applications link to
a user space library that provides local access to data and a OS kernel
module provides coarse-grained allocation and protection. Applications can
modify file data directly while contact the file system service to update
metadata.

Aerie consistos of three components: an untrusted user mode library (libFS),
a trusted file system service (TFS), and a kernel module SCM manager. libFS
handles data access, TFS provides service for metadata updates and concurrency
control among processes, and SCM manager provides storage allocation mechanism.
libFS uses RPC to communicate with TFS, and TFS works as lock manager to 
assign locks to applications using libFS, and it can revoke locks if needed.

The protection is provided by both SCM manager and TFS, which divide the NVMM
into extents, embedding them with permission information, and grant applications
to the extents accordingly. To reduce RPC communication overhead, Aerie
uses a batched and delayed metadata update mechanism, only when updates need to
be visible to other clients. Another optimization Aerie does is to using
a in-memory cache of path names to speed up name resolution.

Aerie designs a file system, PXFS wich complies with POSIX semantics.
To further show the impact of POSIX interfaces on performance, they also design a flat
key-value store storage system (FlatFS), which applications access file
through a simple put/get/erase interface. FlatFS is simpler, avoiding the
overhead of name resolution and accelerates file sharing. The evalution shows FlatFS
outperforms PXFS by up to 109\%, leading us to rethink the file system interface design.

Echo~\cite{echo} explores the NVMM with key value stores. Echo is a lightweight,
persistent key value store system. The data is stored persistent in NVMM,
while DRAM is a thin layer to offset the performance and wear-out weakness of
NVMM.

Echo supports concurrency, data consistency, recoverability and scalability
by employing transactions and snapshot isolation. The database of Echo is
organized as a sequence of snapshots. A snapshot consists of a unique, monotonically
increasing snapshot number and a set of key-value pairs. Each transaction
commitment creates a new snapshot with a new snapshot number. 

To reduce the NVMM writes, Echo uses worker thread, each with a local store
space in DRAM to keep uncomitted data.
When a thread completes transaction, it commits the modified
data to the master store in NVMM, and make the changes visible to other
worker threads.

Echo uses hashtable to lookup version table of a key in NVMM. Each version table
consists of a vector of version table entries that maintains the history
of committed values of the key. With version table it is easy to track
the history and roll back to old versions. Unfortunately Echo does not
resolve the conflicts, aborts and rollbacks support, which is essential
for the system.

To ensure system persistency during system failure, all the NVMM systems
use CPU cache flushing~\cite{PMFS, nvheaps}, logging~\cite{PMFS, nvheaps,
mnemosyne} and hardware primitives~\cite{BPFS, PMFS} to provide persistency.
All these operations introduce overhead and undermine NVMM performance.
To reduce the cost of persistence for NVMM heaps, ~\cite{NVMCap} proposes
several optimizations to current NVMM systems.

~\cite{NVMCap} divides the applications into two categories: those
use NVMM for capacity without persistence (NVMCap), and those use the
persistence (NVMPersist). ~\cite{NVMCap} provides two different
categories of NVMM APIs to these two kinds of aaplications: the NVMCap
applications do not require data persistence and hence do not flush state
of cache, while NVMPersist applications frequently flush cache to obtain
durability guarantees.

Current systems keep metadata in NVMM to recovery from system failure.
These metadata are updated frequently during system running and make
them persistent adds allocator overhead. To overcome the issue, ~\cite{NVMCap}
keep metadata in DRAM buffer, and log the updates to NVMM to reduce NVMM
writes. To further improve the allocator performance, the system allocates
physically contiguous pages in batch and pre-allocate them to applications 
to reduce cache sharing impact. While still using logging for NVMPersist
applications, the logging is also divided into two categories: fine-grained
word log for metadata updates, and coarse-grained object log for object
updates. word log contains the full metadata, while object log indicates
the address and size of the object. This reduces log size and NVMM writes for
object as eliminates the need to log the entire object.
