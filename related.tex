\section{Related Work} 
\label{sec:related}

\begin{table}
	\resizebox{\linewidth}{!}{
	\footnotesize
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
	\hline
%	\multirow{4}{*}{}&PMFS &\DAChell{}\\\hline
%	&\multicolumn{2}{c|}{(Ops per second)}\\\hline
	System & In-place & Logging & COW & clflush & mfence\\\hline
	\hline
	BPFS & * & No & Yes & No & Yes  \\\hline
	PMFS & Yes & Yes & No & Yes & Yes  \\\hline
	Rio Vista & No & Yes & No & N/A & N/A  \\\hline
	Mnemosyne & * & Yes & Yes & Yes & Yes \\\hline
	NV-heaps & No & Yes & No & No & Yes  \\\hline
	CDDS & No & No & Yes & Yes & Yes  \\\hline
	UBJ & Yes & Yes & Yes & No & Yes  \\\hline
	eNVy & No & No & Yes & No & Yes  \\\hline
%	Kiln & Yes & No & No & No & No  \\\hline
	\hline
	Native & Yes & No & No & No & No  \\\hline
	\end{tabular}}
	\vspace*{1mm}
	\caption{\figtitle{Comparison of NVMM system software. * means in-place updates are only performed for memory stores to a single variable or at the granularity of cache line size.}}
	\label{table:comparison}
\end{table}

Table~\ref{table:comparison} lists the mechanisms adoption status of some
systems we have introduced. Almost all the NVMM systems rely on logging or
copy-on-write to provide atomicity, and clflush and/or mfence to guarantee
consistency. (Rio Vista does not mention using clflush or mfence, but we believe
it should.) NVMM software must sacrifice some performance to achieve
atomicity and consistency.

The emergence of NVMM has motivated many projects, not limited to the categories
we have introduced. In this section we will talk about related researches
on NVMM system software design.

\subsection{System design} 
\label{sec:systemdesign}

~\cite{systemimplications} shows the implications for a NVMM-only
system. It addresses several changes and challenges to current system design,
such as paging and application running model. A NVMM-only system may benefit
from the persistency of NVMM for fast recovery, but also meets the security
and privacy issue which current system does not have. How to design a
NVMM-only system is still in exploration.

To overcome the CPU cache flush and write re-ordering issues, researchers
have proposed various solution, such as explicit CPU cache flushing, logging
and hardware primitive support. However, these solution are difficult
to implement with legacy software and hardware, and they also add extra
overheads. Some researches try to find a simple and low-overhead solution.

\textbf{WrAP}~\cite{WrAP} proposes a system architecture
 that provides persistence
ordering, persistence atomicity and persistence protection guarantees. 
WrAP consists of three components: a victim persistence cache (VPC); a log
area of NVMM that keeps log of update operations and an asynchronous channel
that used to propagate slow log records to NVMM. WrAP provides a lightweight
firewall prevents arbitrary writes to NVMM and a non-intrusive interface
for interaction between the cache system and NVMM.

Every write to NVMM in WrAP results in two update paths: an atomic update to
the log area in NVMM, and a normal store instruction to the NVMM address. Both
paths are performed simultaneously. VPC hold PCM entries that are evicted
frpm the last-level cache and serves as the final backing store for evicted
variables. VPC does not need to be persistent.
Finally, the update to the home locations will be made
independently by a COPY module that is part of the WrAP controller. 
By propagate the write operation in two paths, WrAP not only makes the 
write atomic, but also avoid the overhead of CPU cache flushing.
Unfortunately WrAP does not provide evaluation of their system.

\textbf{SoftWrap}~\cite{softWrAP}
 is the software implementation version of WrAP.
Instead of applying a WrAP controller and VPC, SoftWrAP installs a library
to transform applications to use the WrAP APIs to access the NVMM. WrAP
uses VPC to hold PCM entries evicted from CPU cache, while SoftWrAP
uses alias update, routes the cache entries to write to a different location
without affecting the original data. SoftWrAP achieves the same persistent
guarantee as WrAP without hardware modification.

\textbf{Kiln}~\cite{Kiln} provides a persistent memory design
 that adopts a non-volatile
cache and a non-volatile main memory to enable atomic in-place updates without
logging or copy-on-write. Kiln moves the transaction commitment to the NV
cache layer, all the data are resided inside CPU cache (including last level
NV cache) before committed. When commit a transaction, Kiln flushes all
the dirty lines in volatile cache to NV cache, and then change the states of
NV cache lines to committed. Committed NV cache lines are persistent and
can be flushed to NVMM. Kiln has a similar programming interface as NV-Heaps
and Mnemosyne, but the majority contribution is the NV cache layer.

\subsection{Recovery} 
\label{sec:recovery}

Several early researches focus on how to recovery the system with NVMMs.
\textbf{RVM}~\cite{RVM} is an efficient,
 portable and easily used implementation of
recoverable virtual memory for Unix enviorments. By eliminating the support
for nesting, distribution and concurrency control, the implementation of 
RVM becomes very simple and easy to use. RVM manages recoverable memory in
segments, each with a file or a disk partition as backing store. Applications
calls the interfaces provided by RVM to begin and commit transactions. RVM
uses a undo/redo logging strategy to make the persistency guarantee. Old values
are copied to undo log, while new values are written to the redo log. Applications call log control commands to force redo log to write into the disks
and become persistent.

\textbf{WSP}~\cite{WSP} stands for whole-system persistence.
 It aims at systems where
all memory is NVMM, and it transparently recovers an application's entire state,
making a system failure appear as a suspend/resume event. WSP does not
do anything about logging or explicit CPU cache flushing; instead, it does 
\emph{flush-on-fail}, which flushes CPU registers and cache lines to NVMM
on system failure, using a small residual energy window provided by system
power supply. As WSP does not flush during normal operations, it eliminates
the runtime overhead of flushing CPU cache lines on each update entirely, also
it does not require any software modification such as NV-heaps~\cite{nvheaps}
 and Mnemosyne~\cite{mnemosyne}.

WSP consists of NVMM devices, a hardware power monitor, and software suspend/resume routines. When the WSP system fails, the hardware power monitor triggers
an interrupt to the CPU core. All the CPU cores flush their cache lines to the 
NVMM and then halts. On the recovery, resume routine does the inverse to recover
the system to the consistent state. As WSP eliminates the runtime persistency
operations, it shows good performance comparing to NV-heaps.

A remaining issue of WSP is device recovery. As the device drivers are restored
but the devices lost power and need to re-initialize, the state of software
and hardware is not consistent. WSP proposes to clean up device state on
the resume path, or virtualize the devices by using a VM hypervisor.


\subsection{Protection} 
\label{sec:protection}

As NVMM is sitting on the system memory bus and expose the memory range
to the applications, how to protect it from stray writes become a issue.
\textbf{PMBD}~\cite{PMBD}
 is an attempt to leverage the existing modules that the 
operating system provides and makes the system light-weight and easy to
implement. Rather than the systems we have seen,
PMBD treats NVMM as a block device sitting on the memory bus. The advantage
is that it is easy to provide protection and order persistence for
a block device than a memory device.

PMBD proposes several solutions to protection and order persistence and
compare the performance. For protection, one solution is to disable page
table entry for write until needed; however, this incurs big penalty on
TLB flush and performance is bad. Buffer the write requests and commit
them as a batch helps to improve the performance for sequential writes,
but not for random writes. The author finds out the best solution is 
\emph{private mapping}: NVMM pages are mapped to virtual space only when
required and unmapped after write finishes. Evaluation shows private
mapping provides 90\% bandwidth of the optimal case. Private mapping
addes overhead for each write request, but reduces page table size and
TLB pollution.

\ignore{
To guarantee write order, PMBD compares two solutions: use a non-temporal
store (\texttt{movntq}) to bypass the CPU cache and \texttt{sfence} to
flush the buffered data, or use \texttt{clflush} to flush CPU cache
and \texttt{mfence} to ensure the data is written back. With micro-benchmark
tests \texttt{movntq/sfence} generally works better than \texttt{clflush/mfence}
solution.
}

PMBD provides protection of NVMM with the cost of performance.
The fact that
PMBD treats NVMM as a block device incurs significant overhead
as Linux block layer is designed for slow storage devices.
Also PMBD does not allow applications to access NVMM directly.
We don't think PMBD is the right solution to utilize NVMM, but it provides
some insights on NVMM protection, which is not addressed by system such as BPFS.

\ignore{
\textbf{CDSMM}~\cite{CDSMM} also presents the memory management module design
 for NVMMs.
Rather than performance, it focuses on security and protection: it introduces
techniques for 1) robust wear-aware memory allocation, 2) preventing for 
erroneous writes, and 3) consistency-perserving updates that are cache-efficient.

As hardware wear-leveling has performance issues, CDSMM presents a
software memory allocator that always allocate new blocks rather than reusing
old freed blocks to average the writes to each block. To reduce metadata
writes, CDSMM puts important information in DRAM and sync to NVMM only required.It also adds checksum to prevent stray writes from modifying metadata.
}

\textbf{CDSMM}~\cite{CDSMM} presents the MMU design for NVMMs, focuses on
security and protection. It protects NVMM pages with system call
\texttt{mprotect}, but performs it in a batch way so that to reduce mode
switch overhead, kernel structure modifying overhead and TLB flushes.
CDSMM also does a software wear-leveling by always allocating new blocks
rather than reusing old freed blocks to average the writes to each block.

The problem of CDSMM is that it requires both software and hardware
modifications, and adds significant overhead to software programming.

\ignore{
 To reduce metadata
cache line so that application can use command to track data is either in 
CPU cache or persistent on NVMM, and resolves the persistency and write ordering
issue. 

The disadvantage of CDSMM is that it requires both software and hardware
modifications, adds significant overhead to software programming. The paper
shows a B+ tree example and in order to apply CDSMM design, it does not only 
adds much more software efforts, but also change the definition of B+ tree:
the values in a node is not in sorted order and it takes linear time to search
value in a node. CDSMM has several interesting ideas about NVMM protection, but
to integrate the design in applications is still difficult.
}

\subsection{Wear leveling}
\label{sec:wear}
As NVMM has a lower endurance than DRAM, frequent writes will wear out
NVMM in a few years and makes it unusable. Without wear leveling, the lifetime
of a PCM based NVMM is just 0.5 to 3 years. This is unacceptable in a real
system. ~\cite{PCMHierarchy} introduces write line random shifting to uniform
the writes. ~\cite{PCM_EfficientMainMemory} tracks the redundant bit-writes to
eliminate the writes that do not change the value. It also uses byte-level row
shifting to rotate the writes at fine-grained granularity, as well as segment
swapping at coarse-grained granularity.

Implement wear leaveling in software has several disadvantages: first, it makes
the software more complex. Second, software needs additional space to store
the mapping data. If the 
However, researches propose a smaller shift granularity such as line size
or byte level.
At this shift level, the mapping information would consume much more space.
Third, Software wear leveling cannot be more efficient
than a hardware solution. We conclude that NVMM wear leveling should be
implemented in hardware. Start-Gap~\cite{startgap1} is a simple and effective
wear leveling technique for PCM-based NVMM. It uses only two registers and
makes NVMM robust to malicious attacks. \cite{secrefresh} and \cite{freep}
are another two hardware solutions to balance the writes to PCM and improve
lifetime.

\subsection{Rethink the POSIX File Interface}
\label{sec:noposix}

NV-heaps and Mnemosync provide low overhead interface to access NVMM. However,
they require application modifications.
On the other hand, with the Both \DAChell{} and \CChell{} transparently
 provide a POSIX-compatible
interface, so no changes to the application are needed.  This means, however,
 that applications must use POSIX-style file access functions and some of these
 are a poor match for NVMM.
In particular, the POSIX interface
requires the programmer to specify the buffer that \texttt{read()} copies data to.
POSIX places no constraints on the size
or alignment of the buffer, so a copy operation is usually unavoidable.  For
slow disks and even SSDs, the cost of this copy is small.

\cfigure[Graphs/latency-break-quill.pdf,{\figtitle{Chell-Direct 4~kB read/write latency breakdown. Memory copy accounts for 76\% of total latency. Page table is populated on file open, so the page fault overhead is eliminated.}},fig:latency-break]

But for NVMM based system such as \DAChell{} and \CChell{}, the relative cost
 is much larger.
Figure~\ref{fig:latency-break} shows the latency breakdown of 4~kB read and
write operations of \DAChell{} with page table populated and page fault overhead
eliminated.  The memory copy accounts for 76\% for 4~kB
read and 77\% for 4~kB write. For \CChell{}, the situation is similar.
This high overhead of memory copying makes the POSIX interface
itself a major impediment to exploiting the performance of NVMM.

To eliminate it, we propose a new file read interface optimized for
NVMM-based storage systems.  The interface provides a new function:

\vspace{1em}
\begin{tabular}{l}
ssize\_t \grb{}(int fd, char **buf, size\_t count);\\
%ssize\_t \gwb{}(int fd, char **buf, size\_t count);\\
\end{tabular}
\vspace{1em}

\ignore{How do writes work in this interface?  There's something missing here:  \gwb{} doesn't actually have any input data, right?  This gets me the address, but then I have to copy the data there, I guess.  That means I need to ensure that that memory won't go anywhere while I'm writing, and I need to make some other call to make sure that durability is ensured.  I think the best bet would be to just worry about reads, especially given the time remaining.}
\ignore{Yes. write is much more complicated.}

The interface is similar to the conventional \texttt{read}, except that calls
to \texttt{\grb{}} return a buffer to programmer via the \texttt{buf}
parameter.  The \texttt{*buf} contains a portion of the target file
(\texttt{fd}), starting from current file offset and extending for up to
\texttt{count} bytes.  The return value is the actual length of the memory
region.  No copying is necessary.  We added the \grb{} interfaces based to
\DAChell{}.

\ignore{
\begin{figure}[htb]
\includegraphics[width=\linewidth]{Graphs/mmap.pdf}
\caption{mmap performance}
\label{fig:mmap}
\end{figure}
}


\cfigure[Graphs/noposix.pdf,{\figtitle{\grb{} interface latency for different request sizes: latency of PMFS and \DAChell{} increases with request size, while \grb{} has constant latency}},fig:noposix]

\ignore{
Figure~\ref{fig:mmap} compares the performance of \grb{} and \texttt{read}
for files stored in NVMM with PMFS.  Conventional \andiry{\texttt{read} is faster for request sizes
below 8~kB WHy is this?}, but \grb{} outperforms \texttt{read} by up to 366\%. This is
because \grb{} calls \texttt{mmap()} for each request\andiry{why?}, for small
request the overhead of mmap operation (about 1000~ns) is larger than the
\texttt{memcpy} operation performed by \texttt{read} (about 400~ns for 4~kB) , but
for larger requests, the overhead of \grb{} is a
page fault to setup the persistent memory page in the page table
(about 100~ns), which is much smaller than a 4~kB \texttt{memcpy} of POSIX
read. \andiry{This seems like a pretty inefficient way to do this.  If the file is already in memory, for \DAChell{}, the latency for \grb{} should be just the lookup time in the btree}
}

Figure~\ref{fig:noposix} compares the performance of the normal POSIX interface
and \texttt{\grb{}} for a sequential read workload running on PMFS with
\Chell{} enabled.  The copy that the POSIX interface requires causes latency to
increase linearly with request size.  \texttt{\grb{}} does not do any copying, so
latency is nearly constant.  For 4~kB request, \grb{} outperforms POSIX for
4.2\x{}, and for 1~MB request, \texttt{\grb{}} achieves performance gain up to
700\x{}.


