\section{Future Work} 
\label{sec:future}

NVMM brings us with fast, dense,
non-volatile, and byte-addressable memory. With these properties NVMM may
replace DRAM or slower storage in the future.
This may greatly change the current DRAM/storage
hierarchy and impact the system software design.

If NVMM replace DRAM entirely, all the programs will be executed on a
persistence address space, and whole system persistency is available~\cite{WSP}.
This may reduce system recovery time to seconds and programs can resume
running form where they are interrupted. However, putting everything in
NVMM also has risk. A malicious attack can break the application data, and if
the operating system cannot detect the attack, applications may never run
properly again, as writing to NVMM is persistent and cannot be fixed by a reboot. 

On the other hand, if NVMM replace the slower storage devices, the DRAM/storage
architecture need to be redesigned. Several common operating system features
may be obsoleted or needs modficiation:
\begin{itemize}
\item \textbf{Is file system still necessary?}
Disks and other storage devices do not provide any
protection mechanism and rely on file system to protect the data, but NVMM
can be protected by MMU hardware. A file system resides in the kernel and
talks to the virtual file system layer, which adds too much overhead to access
NVMM, and frequent metadata change also undermines performance. We argue that
traditional kernel file system is not suitable for NVMM. 
\item \textbf{Allocation granularity}
Current operating systems allocate DRAM with 4KB pages by default,
and most NVMM system resues the size for NVMM allocation. Superpage argues
that large page size such as 2MB reduces TLB misses, but it also yields
space waste and coarse-grained protection. We assume NVMM should be allocated
by extents, which provides both flexibility and fine-grained control. However,
this means the TLB based protection needs to be redesigned.
\item \textbf{Decouple protection from allocation} DRAM is organized by 
page table in the operating system. This means both the allocation and
protection of DRAM is at page-level granularity. However, this not flexible
enough to manage NVMM. If we store multiple files smaller than page size,
page-level protection means they cannot reside in the same page and hence
the space is wasted. A big file which spans multiple pages have a single
access permission, and enforce the protection to each page is also a waste.
Decouple the protection from allocation makes NVMM management much more
 flexible.
\item \textbf{Impact to current memory system}
With NVMM, DRAM file cache does not exist anymore. With NVMM as file cache,
data is persistent and does not need to sync to disk periodically, eliminates
the expensive sync calls.
Also, operating system does not swap out pages to disk when memory is low;
Instead it swaps DRAM pages to NVMM, or it can borrow NVMM pages directly.
\item \textbf{New APIs for NVMM access}.
As pointed out in Section~\ref{sec:noposix}, current POSIX file 
interface may impace performance when accessing data on NVMM.
The works described in Section~\ref{sec:access} provide their own interface
for applications to access NVMM efficiently. As there is no standard, each of
them proposes different APIs and applications need to be modified accordingly.
We assume there should be a new standard API for NVMM access that both
applications and NVMM software can follow, and that will save lots of efforts
in software implementation. 
\end{itemize}

Also, POSIX APIs trigger system calls, which results in mode switch overheads.
This is not a problem when access slow storage devices, but for fast NVMM things
are different.
Researchers have realized the software overheads could impact
fast storage and networking device performance,
and they have proposed different approaches to make the common path as fast
as possible.
Arrakis~\cite{Arrakis} and IX~\cite{IX} argues that seperating data plane
from control plane could improve application performance by accelerating
data accesses. MonetaD~\cite{monetad} and BankShot~\cite{BankShot} access
SSD devices directly from user space to eliminate the mode switch overhead.
FlexSC~\cite{FlexSC} tries to reduce the system call overhead,
and works like MegaPipe~\cite{MegaPipe} tries to move work to user space and
batch system calls to reduce mode switches. Volos~\cite{system-scm} argues
NVMM file system should be implemented largely as a user space library, rather
than as a kernel-level service. All the naming and indexing should be provided
in user-mode, the kernel is responsible only for NVMM protection and permission
management.

Currently NVMM researches are focused on single machine system. However,
we believe NVMM can have its place on distributed systems.
RAMCloud~\cite{RAMCloud} proposes a data center with all the data reside in DRAM, but it still needs to write the data back in disk in case of crash recovery.
With NVMM, that is not necessary. Mojim~\cite{mojim} claims that the NVMM page
replication can be fast and efficient among the nodes in a distributed system.
We are expecting more researches on NVMM usage in distributed system.

