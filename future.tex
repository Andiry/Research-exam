\section{Future Work} 
\label{sec:future}

NVMM brings us with fast, dense,
non-volatile, and byte-addressable memory. With these properties NVMM may
replace DRAM or slower storage in the future.
This may greatly change the current DRAM/storage
hierarchy and impact the system software design.

If NVMM replace DRAM entirely, all the programs will be executed on a
persistence address space, and whole system persistency is available~\cite{WSP}.
This may reduce system recovery time to seconds and programs can resume
running form where they are interrupted. However, putting everything in
NVMM also has risk. A malicious attack can break the application data, and if
the operating system cannot detect the attack, applications may never run
properly again, as writing to NVMM is persistent and cannot be fixed by a reboot. 

On the other hand, if NVMM replace the slower storage devices, the DRAM/storage
architecture need to be redesigned. Several common operating system features
may be obsoleted or need modifications:
\begin{itemize}
\item \textbf{File system design}
Current NVMM file systems still work like disk file systems in many ways.
However, the huge differences between disks and NVMM make us rethink file
system design for NVMM.
Disks file systems provide protection as storage devices do not have any
protection mechanisms, but NVMM can be protected by MMU hardware.
Traditional file systems are hierarchical
that provide locality disks can make use of, while NVMM file systems do not
need to be organized in this way.
Seltzer et al.~\cite{Seltzer} claims that
existing hierarchical file systems are dead and files should be organized by
how they are accessed instead of by names, provides insight into NVMM file
system design.
\item \textbf{Allocation granularity}
Current operating systems allocate DRAM with 4KB pages by default,
and most NVMM system reuses the size for NVMM allocation. Superpage argues
that large page size such as 2MB reduces TLB misses, but it also yields
space waste and coarse-grained protection. We assume NVMM should be allocated
by extents, which provides both flexibility and fine-grained control. However,
this means the TLB based protection needs to be redesigned.
\item \textbf{Decouple protection from allocation} DRAM is organized by 
page table in the operating system. This means both the allocation and
protection of DRAM is at page-level granularity. However, this is not flexible
enough to manage NVMM. If we store multiple files smaller than page size,
page-level protection means they cannot reside in the same page and
the space is wasted. A big file which spans multiple pages have a single
access permission, and enforce the protection to each page is also a waste.
Decouple the protection from allocation makes NVMM management much more
 flexible.
\item \textbf{Impact to memory management unit}
With NVMM, DRAM file cache does not exist anymore. With NVMM as file cache,
data is persistent and does not need to sync to disk periodically, eliminates
the expensive sync calls.
Also, operating system does not swap out pages to disk when memory is low;
Instead it swaps DRAM pages to NVMM, or it can borrow NVMM pages directly.
\item \textbf{New APIs for NVMM access}.
As pointed out in Section~\ref{sec:noposix}, current POSIX file 
interface may impace performance when accessing data on NVMM.
The works described in Section~\ref{sec:access} provide their own interfaces
for applications to access NVMM efficiently. As there is no standard, each of
them proposes different APIs and applications need to be modified accordingly.
We assume there should be a new standard API for NVMM access that both
applications and NVMM software can follow, and that will save lots of efforts
in software implementation. 
\end{itemize}

Researchers have realized the software overheads could impact
fast storage and networking device performance,
and they have proposed different approaches to make the common path as fast
as possible.
Arrakis~\cite{Arrakis} and IX~\cite{IX} argues that seperating data plane
from control plane could improve application performance by accelerating
data accesses. Moneta-D~\cite{monetad} and BankShot~\cite{BankShot} access
SSD devices directly from user space to eliminate the mode switch overhead.
FlexSC~\cite{FlexSC} tries to reduce the system call overhead,
and works like MegaPipe~\cite{MegaPipe} tries to move work to user space and
batch system calls to reduce mode switches. Volos et al.~\cite{system-scm}
argues
NVMM file system should be implemented largely as a user space library, rather
than as a kernel-level service. All the naming and indexing should be provided
in user-mode, the kernel is responsible only for NVMM protection and permission
management.

Currently NVMM researches focus on single machine system. However,
we believe NVMM can have its place on distributed systems.
RAMCloud~\cite{RAMCloud} proposes a data center with all the data reside in DRAM, but it still needs to write the data back in disk in case of crash recovery.
With NVMM, that is not necessary. Mojim~\cite{mojim} claims that the NVMM page
replication can be fast and efficient among the nodes in a distributed system.
We are expecting more researches on NVMM usage in distributed system.

